* 웹문서에서 정보 추출하기

웹 페이지를 수집한 후에, 웹 페이지 내에서 원하는 정보를 추출하는 방법에 대해서 알아봅니다.


** HTML 파일의 구조

HTML은 Hyper-text Markup Language로, 문서 표현 언어의 하나입니다. 인터넷 문서들의 대부분이 이 언어로 이루어져 있죠.

HTML의 구조를 이해하면 필요한 정보를 추출하는데 도움이 됩니다.


#+BEGIN_SRC html
<html>
    <head>
    </head>

    <body>
        <div id="main-container" style="color: black;">This is a container</div>
        <div class="article header">Webpage scrapying</div>
    </body>
</html>
#+END_SRC


** 태그

HTML은 전반적으로 태그로 이루어져 있습니다. 태그란 =<html></html>= 처럼, 괄호(=<>=) 사이에 태그 이름이 들어가고, 태그는 태그 열기(=<html>=)와 태그 닫기(=</html>=)로 이루어져 있습니다. 태그 사이에는 다른 태그가 들어올 수도 있고 최종 문자열 값이 들어올 수도 있습니다.

태그의 종류에는 여러 가지가 있는데, 웹 페이지에서 정보를 추출할 때 눈여겨볼만한 유용한 태그는 다음과 같은 것들이 있습니다.

 - div
 - span
 - p

태그에는 속성이 기재될 수 있습니다. 속성은 태그명 뒤에 자리잡고, 속성명과 값으로 이루어져 있습니다. (ex. ~id="main-container"~)

웹 페이지 추출에서 유용한 속성의 종류에는 다음과 같은 것들이 있습니다.

 - id
 - class
 - style


** CSS Selector

커다란 HTML 문서에서 내가 원하는 정보가 있는 위치를 어떻게 지칭할 수 있을까요? 모든 줄을 검사하면서 특정 문자열이 포함된 줄을 포착할 수도 있겠지만, 조금 더 쉬운 방법들이 있습니다. 그중, 웹에서 jquery 라이브러리가 등장한 이후로는 CSS selector를 지정하는 방법이 많이 사용됩니다.

가장 많이 활용되는 CSS selector 의 기본 형태는 아래와 같은 것들이 있습니다:

 - ~div~: 모든 =div= 태그를 선택
 - ~div#main-container~: =div= 중, =id= 가 =main-container= 인 =div= 만을 선택
 - ~div.header~: =div= 중, =class= 가 =header= 인 =div= 들만을 선택
 - ~div[style="color: black;"]~: =div= 중, =style= 이라는 속성이 =color: black;= 이라는 값을 가지는 =div= 만을 선택

그런데, 위의 selector만으로는 지정하기 어려운 경우가 있습니다. 아래와 같은 [[https://www.w3.org/TR/css3-selectors/#combinators][combinator selector]]를 사용하면, 웬만한 경우는 처리할 수 있습니다:

 - ~div.header li~: =header= 클래스를 가지는 =div= 태그 하위에 있는 모든 =li= 를 선택
 - ~div.header > ul~: =header= 클래스를 가지는 =div= 태그의 1단계 하위에 있는 모든 =ul= 를 선택
 - ~div.header li:nth-child(2)~: =header= 클래스를 가지는 =div= 태그 하위에 있는 태그 중, =li= 가 2번째 자식인 모든 =li= 를 선택
 - ~div.header li:first-child~: =header= 클래스를 가지는 =div= 태그 하위에 있는 태그 중, =li= 가 첫번째 자식인 모든 =li= 를 선택
 - ~div.header li:last-child~: =header= 클래스를 가지는 =div= 태그 하위에 있는 태그 중, =li= 가 마지막 자식인 모든 =li= 를 선택


#+BEGIN_SRC html
<html>
    <head>
    </head>

    <body>
        <div id="main-container" style="color: black;">This is a container</div>
        <div class="article header">Webpage scrapying <a href="http://jsoup.org">Visit JSoup!</a></div>
    </body>
</html>
#+END_SRC

위의 HTML 코드에서, =This is a container= 라는 문자열을 지칭하는 CSS selector는 아래와 같습니다.

#+BEGIN_SRC css
div#main-container
#+END_SRC

=div= 태그 중에서, =main-container= 라는 id 값을 가지고 있는 것을 지칭합니다.

=Webpage scrapying= 이라는 문자열을 지칭하는 CSS selector는 아래와 같습니다.


#+BEGIN_SRC css
div.article
#+END_SRC

그 중에서 =Visit JSoup!= 이라는 문자열을 지칭하는 CSS selector는 아래와 같습니다.

#+BEGIN_SRC css
div.article > a
#+END_SRC


** BeautifulSoup

[[https://www.crummy.com/software/BeautifulSoup/bs4/doc/][BeautifulSoup]]은 웹 페이지 내에서 원하는 문서 구성 요소를 CSS selector 형식으로 특정할 수 있도록 도와줍니다.

BeautifulSoup은 외부 라이브러리이지만, Anaconda에 기본적으로 포함되어 있기 때문에 별도로 설치할 필요는 없습니다. Anaconda를 사용하지 않고 순수 Python 배포본을 사용하는 경우에는 아래와 같이 설치할 수 있습니다.


#+BEGIN_SRC sh
pip install beautifulsoup4
#+END_SRC


** 연습문제

연습문제로 아래 URL의 HTML에서 정보를 추출해보겠습니다. 아래의 URL은 다음 아고라의 주소입니다. 여기에서 글 제목과 글쓴이, 글의 URL 주소를 가져오겠습니다.

http://bbs3.agora.media.daum.net/gaia/do/petition/list?bbsId=P001&objCate1=1

우선 위의 URL에 접속한 후, Chrome에서 개발자 도구를 엽니다. Ctrl-Shift-I를 누릅니다. Elements 탭에서 돋보기 아이콘을 선택한 후, 확인하고자 하는 HTML 요소를 클릭합니다. 해당 요소를 특정할 수 있는 태그 및 속성을 확인합니다.

글 제목을 클릭해보면, =span= 이라는 태그가 =sbj= 클래스(~<span class="sbj">~)를 가지고 있는 것을 볼 수 있습니다. 그리고 그 아래에 =a= 태그에 제목 문자열이 들어있습니다. 따라서 제목을 지칭하는 CSS selector는 다음과 같이 쓸 수 있습니다.


#+BEGIN_SRC css
span.sbj > a
#+END_SRC

개발자 도구의 Console 탭에서 ~$$('span.sbj > a')~ 라고 입력해봅시다.

이와 비슷하게, 글쓴이를 지칭하는 CSS selector는 다음과 같이 쓸 수 있습니다.

#+BEGIN_SRC css
span.sbj > span.name > a
#+END_SRC

개발자 도구의 Console 탭에서 ~$$('span.sbj > span.name > a')~ 라고 입력해봅시다.


이러한 CSS selector를 사용하여, 아고라 글의 제목과 글쓴이, 글의 URL 주소를 가져오는 코드는 다음과 같습니다.


#+BEGIN_SRC python :results output :exports both
  import requests
  from bs4 import BeautifulSoup

  url = 'http://bbs3.agora.media.daum.net/gaia/do/petition/list?pageIndex=1&bbsId=P001&objCate1=1'

  response = requests.get(url)
  soup = BeautifulSoup(response.content)
  subjects = soup.select('span.sbj > a')
  date = soup.select('span.date')
  counts = soup.select('span.cnt > em')
  writers = soup.select('span.sbj > span.name > a')

  entries = zip(subjects, date, counts, writers)

  for subject, date, count, writer in entries:
      _subject = subject.string
      _date = date.string
      _writer = writer.string
      _count = count.string
      href = subject.attrs['href']

      print('|'.join([_subject, _date, _writer, _count, href]))
#+END_SRC

#+RESULTS:
#+begin_example
문재인 대통령의 파렴치 범죄, 확실한 물증|2017.12.28 11:36|사과사|0|read?bbsId=P001&objCate1=1&articleId=211736&pageIndex=1
공휴일을 유급휴일로 바꾸어야합니다.|2017.12.28 11:17|한가닥의 빛|0|read?bbsId=P001&objCate1=1&articleId=211735&pageIndex=1
강경화 딸.. 국적회복 확인하자..|2017.12.28 10:20|정광수|1|read?bbsId=P001&objCate1=1&articleId=211732&pageIndex=1
벌레같은 교도관들이 뼈를 부러뜨리고 성폭행을 하는 등|2017.12.28 10:06|악덕교도관대청소|2|read?bbsId=P001&objCate1=1&articleId=211731&pageIndex=1
광명 운산고 김Y숙 선생의 명예퇴직을 반대합니다!|2017.12.28 08:57|RainSun|1|read?bbsId=P001&objCate1=1&articleId=211730&pageIndex=1
근로복지공단은 당장 꼼수를 멈추라!|2017.12.28 06:16|풍경소리|4|read?bbsId=P001&objCate1=1&articleId=211729&pageIndex=1
삼성화재보험 중소기업상대로 사기행각|2017.12.28 06:01|이천곤|2|read?bbsId=P001&objCate1=1&articleId=211728&pageIndex=1
학교 규칙이라는 구실로 휴대폰 수거를 하지 말아주십시오|2017.12.28 02:57|전승훈|0|read?bbsId=P001&objCate1=1&articleId=211726&pageIndex=1
지배자들|2017.12.28 01:01|deadkillers-society|1|read?bbsId=P001&objCate1=1&articleId=211725&pageIndex=1
사이보그 이리역 폭파 시멘틱스|2017.12.28 00:11|deadkillers-society|0|read?bbsId=P001&objCate1=1&articleId=211724&pageIndex=1
사이보그 데이터 베이스 킬 입증|2017.12.28 00:09|deadkillers-society|0|read?bbsId=P001&objCate1=1&articleId=211723&pageIndex=1
제2 imf, 살인의 추억 재현될 수 있습니다!|2017.12.27 23:40|deadkillers-society|0|read?bbsId=P001&objCate1=1&articleId=211722&pageIndex=1
[국민감사] 대법원이 국민을 우롱하고 있습니다. 442|2017.12.27 23:33|서재황|0|read?bbsId=P001&objCate1=1&articleId=211721&pageIndex=1
[국민감사] 대법원이 국민을 우롱하고 있습니다. 441|2017.12.27 23:30|서재황|0|read?bbsId=P001&objCate1=1&articleId=211720&pageIndex=1
어린이집 평가인증 부모가 할수 있게해주세요 |2017.12.27 22:27|허브향기ㆀ|0|read?bbsId=P001&objCate1=1&articleId=211719&pageIndex=1
이재용 부회장을 선처해 주십시오. |2017.12.27 21:28|sunny|0|read?bbsId=P001&objCate1=1&articleId=211718&pageIndex=1
교황님 살펴주세요|2017.12.27 20:16|ww8401|0|read?bbsId=P001&objCate1=1&articleId=211717&pageIndex=1
무고죄로 처벌해 주세요|2017.12.27 18:44|tiger|0|read?bbsId=P001&objCate1=1&articleId=211716&pageIndex=1
무고죄로 처벌해 주세요|2017.12.27 18:40|tiger|0|read?bbsId=P001&objCate1=1&articleId=211715&pageIndex=1
고이와 1987, 이걸 누가 샀을까?! 고이비도요?|2017.12.27 17:35|deadkillers-society|0|read?bbsId=P001&objCate1=1&articleId=211714&pageIndex=1
#+end_example

아래의 나무위키 URL에 대해서, 위키 내부간의 하이퍼링크 목록을 추출해보세요.

https://namu.wiki/w/Python


#+BEGIN_SRC python :exports result :results output
  import requests
  from bs4 import BeautifulSoup

  def visit_page(page):
      name, href = page
      url = 'https://namu.wiki' + href
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html5lib')
      link_elements = soup.select('.wiki-inner-content .wiki-link-internal')
      links = set([(elem['title'], elem['href']) for elem in link_elements])
      return list(links)

  page = ('Python', '/w/Python')
  print([name for name, page in visit_page(page)])
#+END_SRC

#+RESULTS:
: ['명령어', '코엑스', '페리아 연대기', '국부론', '넘파이', 'Swift(프로그래밍 언어)', '스택', 'C언어', '파일:xkcdpythonko.png', '2015년', '연세대학교', '킹덤 언더 파이어', '한국', '스팸(몬티 파이선 스케치)', 'Bottle', 'Erlang', 'APAC', '파이선', '비단뱀', 'Pygame', '창조', 'Django', '코드', 'JDK', '오라클', '부산대학교', '필로우', 'C#', '아스키', '인천대학교', 'callback 함수', '웹 프레임워크', 'Pillow', '프레임워크', '액션스크립트', 'rm -rf /', '카이스트', '심즈 4', 'scikit-learn', '고자', '드롭박스', '파일:나무위키+유도.png', '파이톤', 'tkinter', 'Flask', 'Lua', '부르즈 할리파', '추가바람', '의사코드', '파이썬', 'JIT', '상암', '라이브러리', '프로세스', 'reddit', '코더', '프로그래밍 언어', '나무위키:프로젝트', 'MATLAB', '시드 마이어의 문명', '2016년', 'Ruby', 'PyPy', 'Perl', 'Linux', '리눅스', 'PyGame', '우분투', 'C', '누리꿈스퀘어', 'C++', 'NumPy', '스레드', 'Haskell', '파이게임', '스크래피', 'Beautiful Soup', '이스터 에그', '파일:external/regmedia.co.uk/swift_benchmark.jpg', 'Scrapy', 'OpenCV', '문명 4', '자바 가상 머신', '기계학습', 'Sublime Text', '통합 개발 환경', '중국', 'Go', '42', '프로그래머', 'EVE 온라인', '2014년', '스택 오버플로우', '뱀', '국민대학교', '구조체', 'Notepad++', '인스타그램', 'IBM', '몬티 파이선', '한글', 'LISP', 'JAVA', 'Java', '유튜브', 'xkcd', 'WOW', 'R(프로그래밍 언어)', '2017년', '객체 지향 프로그래밍', 'UC 버클리', 'JavaScript', '뷰티플 수프', '월드 오브 탱크', '코루틴', '이클립스(통합 개발 환경)', 'C(프로그래밍 언어)', '비주얼 스튜디오', "Ren'Py", '상명대학교', '구글', 'JVM', '매사추세츠 공과대학교']

이번에는 위의 내용을 응용해서, snowballing 방식으로 웹페이지를 수집해보세요.

#+BEGIN_SRC ipython :session :exports result :results output raw :ipyfile outputs/beautifulsoup-manuwiki-python-map.png
  %matplotlib inline
  import requests
  import networkx as nx
  import matplotlib.pyplot as plt
  from bs4 import BeautifulSoup

  def visit_page(page):
      name, href = page
      url = 'https://namu.wiki' + href
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html5lib')
      link_elements = soup.select('.wiki-inner-content .wiki-link-internal')
      links = set([(elem['title'], elem['href']) for elem in link_elements])
      return list(links)

  def update_edges(graph, page, links):
      for link in links:
          graph.add_edge(page[0], link[0])

  G = nx.Graph()

  seed = [('Python', '/w/Python')]
  visited = set()

  page = seed.pop()
  if page not in visited:
      links = visit_page(page)
      visited.add(page)
      update_edges(G, page, links)
      seed = seed + links

  page = seed.pop()
  if page not in visited:
      links = visit_page(page)
      visited.add(page)
      update_edges(G, page, links)
      seed = seed + links

  pos = nx.kamada_kawai_layout(G)
  plt.figure(figsize=(12, 12))    # 결과 이미지 크기를 크게 지정 (12inch * 12inch)
  nx.draw_networkx_edges(G, pos, alpha=0.1);
  nx.draw_networkx_labels(G, pos, font_family='Noto Sans CJK KR'); # 각자 시스템에 따라 적절한 폰트 이름으로 변경
  plt.show()
#+END_SRC

#+RESULTS:
[[file:outputs/beautifulsoup-manuwiki-python-map.png]]


** TODO Ajax & JSON

요즘 만들어지는 웹사이트들 중에는, HTML로 모두 미리 작성되는 대신, 서버로부터는 데이터만을 받고 웹브라우저에서 동적으로 HTML 문서 구조를 생성하는 경우가 많습니다. 
